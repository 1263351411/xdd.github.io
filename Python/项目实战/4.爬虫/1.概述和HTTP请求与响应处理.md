# 概述和HTTP请求与响应处理

## 概述

* 爬虫，应该称为网络爬虫，也叫网页蜘蛛、网络机器人、网络蚂蚁等。
* 搜索引擎，就是网络爬虫的应用者。
* 大数据时代的到来，所有企业都希望通过海量数据发现其中的价值。所以需要爬取对特定网站、特顶类别的数据，而搜索引擎不能提供这样的功能，因此需要自己开发爬虫来解决。

## 爬虫分类

### 1.通用爬虫

常见就是搜索引擎，无差别的搜集数据、存储、提取关键字、构建索引库，给用户提供搜索接口。  

* 爬取一般流程  
    1. 初始化一批URL,将这些URL放到带爬队列
    2. 从队列取出这些URL，通过DNS解析IP，对IP对应的站点下载HTML页面，保存到本地服务器中，爬取完的URL放到已爬取队列。
    3. 分析这些网页内容，找出网页里面的其他关心的URL链接，继续执行第2步，直到爬取条件结束。
* 搜索引擎如何获取一个网站的URL
    1. 新网站主动提交给搜索引擎
    2. 通过其他网站页面中设置的外链接
    3. 搜索引擎和DNS服务商合作，获取最新收录的网站

### 2. 聚焦爬虫

* 有针对性的编写特定领域数据的爬取程序，针对 某些类别数据采集的爬虫，是面向主题的爬虫

## Robots协议

指定一个robots.txt文件，告诉爬虫引擎什么可以爬取  

* `/`表示网站根目录，表示网站所有目录。
* `Allow`允许爬取的目录
* `Disallow`禁止爬取的目录
* 可以使用通配符  

robots是一个君子协定，"爬亦有道"  
这个协议为了让搜索引擎更有效率搜索自己内容，提供了Sitemap这样的文件。Sitemap往往是一个XML文件，提供了网站想让大家爬取的内容的更新信息。  
这个文件禁止爬取的往往又是可能我们感兴趣的内容，反而泄露了这些地址。

1. 示例：淘宝的robots[http://www.taobao.com/robots.txt](http://www.taobao.com/robots.txt)

    ````txt
    User-agent:  Baiduspider
    Allow:  /article
    Allow:  /oshtml
    Allow:  /ershou
    Allow: /$
    Disallow:  /product/
    Disallow:  /

    User-Agent:  Googlebot
    Allow:  /article
    Allow:  /oshtml
    Allow:  /product
    Allow:  /spu
    Allow:  /dianpu
    Allow:  /oversea
    Allow:  /list
    Allow:  /ershou
    Allow: /$
    Disallow:  /

    User-agent:  Bingbot
    Allow:  /article
    Allow:  /oshtml
    Allow:  /product
    Allow:  /spu
    Allow:  /dianpu
    Allow:  /oversea
    Allow:  /list
    Allow:  /ershou
    Allow: /$
    Disallow:  /

    User-Agent:  360Spider
    Allow:  /article
    Allow:  /oshtml
    Allow:  /ershou
    Disallow:  /

    User-Agent:  Yisouspider
    Allow:  /article
    Allow:  /oshtml
    Allow:  /ershou
    Disallow:  /

    User-Agent:  Sogouspider
    Allow:  /article
    Allow:  /oshtml
    Allow:  /product
    Allow:  /ershou
    Disallow:  /

    User-Agent:  Yahoo!  Slurp
    Allow:  /product
    Allow:  /spu
    Allow:  /dianpu
    Allow:  /oversea
    Allow:  /list
    Allow:  /ershou
    Allow: /$
    Disallow:  /

    User-Agent:  *
    Disallow:  /
    ````

2. 示例马蜂窝tobots[http://www.mafengwo.cn/robots.txt](http://www.mafengwo.cn/robots.txt)

    ````txt
    User-agent: *
    Disallow: /
    Disallow: /poi/detail.php

    Sitemap: http://www.mafengwo.cn/sitemapIndex.xml
    ````

## HTTP请求和响应处理

其实爬取网页就是通过HTTP协议访问网页，不过通过浏览器反问往往是人的行为，把这种行为变成使用程序来访问。  

### urllib包

urllib是标准库，它一个工具包模块，包含下面模块来处理url:
    * urllib.request 用于打开和读写url
    * urllib.error 包含了由urllib.request引起的异常
    * urllib.parse 用于解析url
    * urllib.robotparser 分析robots.txt文件  

Python2中提供了urllib和urllib2。urllib提供较为底层的接口，urllib2对urllib进行了进一步封装。Python3中将urllib合并到了urllib2中，并更名为标准库urllib包。  

* **urllib.request模块**
    1. 定义了在基本和摘要式身份验证、重定向、cookies等应用中打开Url(主要是HTTP)的函数和类。

























